<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>python - My Blog</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "python";
        var mkdocs_page_input_path = "Full stack\\python.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> My Blog
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Welcome to MkDocs</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Financial engineering</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../Financial%20engineering/Derivatives%20cheatsheet/">Derivatives cheatsheet</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../Financial%20engineering/Derivatives%20exercises/">Derivatives exercises</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../Financial%20engineering/Derivatives%20notes/">Derivatives notes</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Full stack</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../Doccen%20policy%20update/">Doccen policy update</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">python</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#important">Important</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#dictionary">dictionary</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#list">list: []</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#common-issue">Common issue</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#virtual-environments">Virtual environments</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#file-probing">File probing</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#file-size">File size</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#reading-files">Reading files</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#using-pd">Using pd</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-with-open">Using with open</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#reading-all-csv-files-using-glob">Reading all csv files using glob</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#other-useful-add-ons">Other useful add ons</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#basics">Basics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#counting-indexing-subsetting-etc">Counting, indexing, subsetting etc.</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#loops">Loops</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-frame-attributes">Data frame attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#displaying-simulation-results-and-plot">Displaying simulation results and plot</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#na">NA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#groupby">groupby</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#subsetting">Subsetting</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#merge-join-append">Merge, join, append</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#numpy-and-maths">Numpy and maths</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#classes-methods-etc">Classes, methods etc.</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#models">Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#a-preprocessing">A. Preprocessing</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#1-polynomial-features-overfitting">1. Polynomial features &amp; overfitting</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-standard-scaling">2. Standard scaling</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#b-model-selection">B. Model selection</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#1-train-test-split">1. Train test split</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-one-hot-encoder">2. One-hot encoder</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#c-models">C. Models</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#1-linear-regression">1. Linear regression</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-regularisation">2. Regularisation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#d-cross-validation">D. Cross validation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#visualisation">visualisation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#graph-styling">graph styling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#helper-functions">helper functions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#subplots">subplots</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#finance">Finance</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataframe">Dataframe</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#multiprocessing">Multiprocessing</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pipeline">Pipeline</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#miscellaneous">Miscellaneous</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#simple-boolean-if-xyz-prints-1">simple boolean, if xyz, prints 1</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#replace">replace</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#normalise-truetrue">Normalise = True[True]</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2f">:.2f</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#for-codes-spanning-multiple-lines">For codes spanning multiple lines</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#in-paths-work-unless-its-followed-by-n">in paths work unless it's followed by n</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#npsearchsorted">np.searchsorted</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Maths</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../Maths/DATA101%20Data%20engineering%20PostgreSQL/">DATA101 Data engineering PostgreSQL</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../Maths/Doccen%20tasks/">Doccen tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../Maths/International%20Finance/">International Finance</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../Maths/Neural%20networks/">Neural networks</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">My Blog</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Full stack</li>
      <li class="breadcrumb-item active">python</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="python">Python</h1>
<h2 id="important">Important</h2>
<h3 id="dictionary">dictionary</h3>
<pre><code>fruits = {
'apple': 1,
'banana': 3
}
</code></pre>
<p>if wanna append a new key-value pair: <strong>use []</strong></p>
<pre><code>fruits['pineapple'] = 26
</code></pre>
<p>for looping, make sure to use for <strong>key, value</strong> in <strong>enumerate([key1, key2])</strong> or dict.<strong>items()</strong></p>
<p><code>.keys()</code> = print all keys</p>
<p><code>.values()</code> = print all values</p>
<p><code>.items()</code> = print all keys and values as tuples</p>
<p>merging dictionaries</p>
<ul>
<li>dictA<code>.update</code>(dictB)</li>
<li>dictA <code>|</code> dict B</li>
</ul>
<p>nested dictionaries -&gt; use more than 1 []: <code>nested_dict[name][last_name]</code></p>
<h3 id="list">list: <code>[]</code></h3>
<p>Set hue = x, legend = False for ombre graphs</p>
<h2 id="installation">Installation</h2>
<p>First must go to the Python directory:</p>
<pre><code class="language-py">cd C:\Users\kalul\AppData\Local\Programs\Python\Python311\Scripts
</code></pre>
<pre><code>pip install pandas numpy seaborn matplotlib ta statsmodels tsfracdiff hmmlearn scikit-learn 
</code></pre>
<pre><code class="language-py"># Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
from IPython.display import display
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings(&quot;ignore&quot;)
plt.rcParams['figure.figsize'] = (10, 3)
sns.set_theme(style = 'darkgrid', font_scale=0.8, palette='viridis')

import statsmodels.formula.api as smf
</code></pre>
<p><code>rich</code> = pretty terminal print outs</p>
<h3 id="common-issue">Common issue</h3>
<p>When import __ as __ -&gt; DLL error, cannot import (happened to matplotlib and seaborn)</p>
<p>-&gt; <strong>Fix</strong>: Window R -&gt; %APPDATA% -&gt; clean all cache</p>
<h3 id="virtual-environments">Virtual environments</h3>
<p>In general, it's suggested to use virtual environments (I highly suggest looking at the official <a href="https://docs.python.org/3/library/venv.html">Python documentation</a>). With this approach, you easily can set up project-specific Python versions (as well as libraries). Easily manageable and the best part: There are lots of tutorials on the internet on how to approach this:</p>
<p>https://www.freecodecamp.org/news/installing-multiple-python-versions-on-windows-using-virtualenv/</p>
<h2 id="file-probing">File probing</h2>
<h3 id="file-size">File size</h3>
<pre><code class="language-py">import os
import pandas as pd

# Check the file size
file_path = 'data.csv'
file_size = os.path.getsize('data.csv')
print(f'File size: {file_size / (1024 * 1024):.2f} MB')

# Count the number of lines
line_count = sum(1 for line in open('data.csv'))
print(f'Number of lines: {line_count}')

# Load the CSV file into a DataFrame (optional)
df = pd.read_csv(file_path)
</code></pre>
<h2 id="reading-files">Reading files</h2>
<h3 id="using-pd">Using pd</h3>
<p><strong>CSV</strong></p>
<pre><code>file_path = 'D:/Github/Python/datasets/baby.csv' 
baby = pd.read_csv(file_path)
</code></pre>
<p>Arguments for read_csv:</p>
<p><code>index_col = "Year"</code> = set the year column as the index</p>
<p><code>header = 1</code> = set the 1st row as the header</p>
<ul>
<li><code>header = None</code> = remove all headers, columns are numbered 0</li>
</ul>
<p><code>names = ['cola, colb']</code> = set column names</p>
<p><code>thousands = ','</code> = character acting as the thousands separator in numerical values.</p>
<p><code>delimiter/sep = ','</code></p>
<ul>
<li><code>sep = r'\s+'</code> = if delimiter is white space</li>
</ul>
<p><strong>json</strong>
<code>pd.read_json()</code></p>
<h3 id="using-with-open">Using with open</h3>
<p>This approach is more general and can be used for any type of file. You have more control over how you process the content, and it doesn't necessarily assume a tabular structure.</p>
<p>If your data is structured and tabular, and you want to perform data analysis, the pandas methods like <code>pd.read_json</code>, <code>pd.read_csv</code>, etc., are often more convenient. If you need more flexibility, or if your data is not in a standard tabular format, using <code>with open</code> allows you to read and process the content in a more customized way.</p>
<pre><code>with open('example.txt', 'r') as file:
content = file.read()
</code></pre>
<p><strong>Zip</strong>
Importing zip files
1. import zipfile package
2. file_path
3. folder to extract the files to (extract)
4. with zipfile.ZipFile(file_path, 'r') as my_zip:
        my_zip.extractall(extract)</p>
<pre><code class="language-py">import zipfile
zip_file_path = 'D:/Github/Python/data100/4homework/hw02A/data.zip'
extracted_dir = 'D:/Github/Python/data100/4homework/hw02A'

with zipfile.ZipFile(zip_file_path, 'r') as my_zip:
    my_zip.extractall(extracted_dir)

# 'r' = read mode
</code></pre>
<h3 id="reading-all-csv-files-using-glob">Reading all csv files using glob</h3>
<pre><code class="language-py">files = glob.glob('../data/forex_light/*.csv')
forex = pd.DataFrame() 
content = [] 
for i in files: 
    df = pd.read_csv(i, index_col='Date', header=0)
    content.append(df) 

forex = pd.concat(content, axis = 1) 
forex.dropna(inplace=True)
forex.index = pd.to_datetime(forex.index)
forex
</code></pre>
<p>==Advanced: os and zipfile: hw2_foodsafety==</p>
<h2 id="other-useful-add-ons">Other useful add ons</h2>
<p><code>data.convert_dtypes()</code> (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html">documentation</a>) conveniently converts data to the most appropriate type</p>
<p><code>data.drop_na()</code> (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html">documentation</a>)</p>
<p><code>data.rename(columns={'old': 'new', 'old2': 'new2'})</code></p>
<h2 id="basics">Basics</h2>
<h3 id="counting-indexing-subsetting-etc">Counting, indexing, subsetting etc.</h3>
<p><strong>Counting unique values and put into a data frame</strong>
<code>value_counts().reset_index()</code></p>
<p><strong>Sort by any column</strong>
<code>data.sort_values(by = 'col')</code>
* Select column A from 'data' and sort by counts, descending
<code>data['A'].value_counts().sort_values(ascending=False).reset_index()</code></p>
<p><strong>Creating an empty column for now</strong>
<code>data['column'] = None</code></p>
<p><strong>Creating a subset of data</strong>
<code>subset = data[data[column] &lt; 345]</code>
<code>data.loc['row', 'col']</code>
* Similar to R but in R no need for .loc
* Can use <code>iloc[]</code> too -&gt; for key difference [[data100 - UC Berkeley#^38e665]]</p>
<p><strong>Creating a 3rd column as the mean/sum/diff of other columns</strong>
<code>data['col 3'] = data[['col 1', 'col 2']].mean(axis=1, skipna=True)</code>
* We MUST specify axis = 1/0 since without it -&gt; axis = 0
* <code>axis = 0</code> = apply column to column (default)
* <code>axis = 1</code> = apply row to row
* see ==12_deflategate.ipynb==</p>
<p><strong>Copy()</strong>
Everytime you create a subset, include <code>.copy()</code> if you want to avoid changing the og data set.</p>
<p><strong>Dropping a column</strong>
<code>.drop('col', axis = 1)</code> = default is axis = 0 which drops a row (rmb 0 = row, 1 = column)
Or we can just type out <code>axis = "columns"</code> or <code>axis = "rows"</code></p>
<p><strong>Create a new column filled with NA</strong>
<code>data['new column'] = None</code></p>
<p><strong>Naming 1 single column</strong>
Use <code>columns = ['name]</code></p>
<pre><code>df_ssn = pd.DataFrame(
    ['data'],
    columns = ['SSN'])
</code></pre>
<h3 id="loops">Loops</h3>
<p><strong>if continue</strong> = move onto the next iteration</p>
<pre><code class="language-py">for strategy_parameters_value in itertools.product(
    *strategy_parameters_values):

    strategy_params = dict(
        zip(strategy_parameters_keys, 
            strategy_parameters_value))

    if (strategy_params['fast_window'] == 
        strategy_parameters['fast_window'][0] and 
        strategy_params['slow_window'] == 
        strategy_parameters['slow_window'][0]) or \
       (strategy_params['fast_window'] == 
        strategy_parameters['fast_window'][1] and 
        strategy_params['slow_window'] == 
        strategy_parameters['slow_window'][1]) or \
       (strategy_params['fast_window'] == 
        strategy_parameters['fast_window'][2] and 
        strategy_params['slow_window'] == 
        strategy_parameters['slow_window'][2]) or \
       (strategy_params['fast_window'] == 
        strategy_parameters['fast_window'][3] and 
        strategy_params['slow_window'] == 
        strategy_parameters['slow_window'][3]):

        strategy_sides = determine_strategy_side(prices, 
            **strategy_params)
    else:
        continue
</code></pre>
<h3 id="data-frame-attributes">Data frame attributes</h3>
<p><strong>Mode</strong>
<code>data['col'].mode()[0]</code> = [0] specifies the first mode</p>
<h3 id="displaying-simulation-results-and-plot">Displaying simulation results and plot</h3>
<ul>
<li>Def the function WITHOUT the input</li>
</ul>
<p>Outside the function</p>
<ul>
<li>Ask for input</li>
<li>results = function()</li>
<li>display(results.head())</li>
<li>sns.plot</li>
</ul>
<p>==Example==</p>
<pre><code class="language-py">def simulated_difference_of_means(dataset):

    &quot;&quot;&quot;Returns: Difference between mean birthweights of babies of smokers 
    and non-smokers after shuffling labels n times&quot;&quot;&quot;

    data = []

    for i in range(1, rep + 1):
        # Shuffling
      dataset['Shuffled Maternal Smoker'] = dataset['Maternal Smoker'] \
            .sample(frac=1).reset_index(drop=True)
      # Find dim
      dim = difference_of_means(dataset, 'Shuffled Maternal Smoker')
      # Update to results table
        data.append({
                'Stimulation': i,
                'Difference of mean': dim
        })

    df_data = pd.DataFrame(data)
    return df_data

# input
rep = int(input('Enter the number of repetitions: '))
# call function &amp; display data frame result
df_data = simulated_difference_of_means(births_subset)
display(df_data.head())
sns.histplot(data = df_data, x = 'Difference of mean')
</code></pre>
<h3 id="na">NA</h3>
<p>To quickly remove na</p>
<p><code>gdp = gdp[~gdp.isna()]</code></p>
<h3 id="groupby">groupby</h3>
<p>Basic syntax: <code>dataframe.groupby['gender']('frequency').mean()</code>. We can</p>
<p><strong>add more groupby criteria</strong>: ==e.g. not just by gender but also by age==</p>
<blockquote>
<p>df.groupby(<strong>['gender', 'age']</strong>)['frequency'].mean()
* key: the use of <strong>[]</strong> when more than 1 element is passed to the ()</p>
</blockquote>
<p><strong>add more groupby metrics of interest</strong>:</p>
<blockquote>
<p>df.groupby('gender')[['frequency', 'count']].agg(<strong>['mean', 'max']</strong>)</p>
</blockquote>
<h3 id="subsetting">Subsetting</h3>
<p>To quickly select rows of this df that is also available in another: <code>A[A.index.isin(B.index)]</code></p>
<pre><code>Y_common = Y[Y.index.isin(Y_fracdiff.index)]
</code></pre>
<h3 id="merge-join-append">Merge, join, append</h3>
<p>Add columns to existing df (usu for features)</p>
<p><code>dfa = dfa.join</code>(dfb, on = 'Date')</p>
<h3 id="numpy-and-maths">Numpy and maths</h3>
<p><strong>creating an array</strong></p>
<pre><code class="language-py">emission_prob = np.array([
    [0.3, 0.5, 0.2], 
    [0.1, 0.5, 0.4]   
])
</code></pre>
<blockquote>
<p>The first number controls the row, second number controls the columns (like dataframes)
To print the entire first row: <code>emission_prob[0, :]</code>        # array([0.3, 0.5, 0.2])</p>
</blockquote>
<p><strong>np.linspace</strong>(1, 100, 77)
Print out 77 evenly spaced numbers from 1 to 100</p>
<h3 id="classes-methods-etc">Classes, methods etc.</h3>
<blockquote>
<p>A <strong>class</strong> in Python is like a blueprint for creating objects (instances). It defines a set of attributes (variables) and methods (functions) that the objects created from the class will have.</p>
</blockquote>
<p>Think of a class as a <strong>template</strong> for creating something. For example, if you have a <code>Car</code> class, it might define that every car has attributes like <code>color</code>, <code>make</code>, and <code>model</code>, and methods like <code>drive()</code> and <code>stop()</code>.</p>
<pre><code class="language-py">class Car:
    def __init__(self, color, make, model):
        self.color = color
        self.make = make
        self.model = model

    def drive(self):
        print(f&quot;The {self.color} {self.make} {self.model} is driving.&quot;)

    def stop(self):
        print(f&quot;The {self.color} {self.make} {self.model} has stopped.&quot;)
</code></pre>
<p>Here, <code>Car</code> is a class. It defines:</p>
<ul>
<li><strong>Attributes</strong>: <code>color</code>, <code>make</code>, <code>model</code></li>
<li><strong>Methods</strong>: <code>drive()</code>, `stop()</li>
</ul>
<blockquote>
<p>An <strong>object</strong> is an instance of a class. When you create an object from a class, it’s like creating a specific item from a blueprint.</p>
</blockquote>
<pre><code class="language-py">my_car = Car(&quot;red&quot;, &quot;Toyota&quot;, &quot;Corolla&quot;
</code></pre>
<p>Here, <code>my_car</code> is an <strong>object</strong> of the <code>Car</code> class. It has its own specific values for <code>color</code>, <code>make</code>, and <code>model</code>.</p>
<blockquote>
<p>A <strong>method</strong> is a function that is defined inside a class and operates on instances of that class. Methods are used to define the behaviors of the objects created from the class.</p>
</blockquote>
<pre><code class="language-py">my_car.drive()
</code></pre>
<p>This calls the <code>drive()</code> method on the <code>my_car</code> object, making it print <code>"The red Toyota Corolla is driving."</code>.</p>
<pre><code class="language-py">def __init__(self, color, make, model):
    self.color = color
    self.make = make
    self.model = model
</code></pre>
<p>In the <code>Car</code> class, <code>__init__</code> sets the initial values for <code>color</code>, <code>make</code>, and <code>model</code> when a new <code>Car</code> object is created.</p>
<blockquote>
<p><strong>Inheritance</strong> allows one class to inherit the attributes and methods of another class. The new class (child) can add new attributes and methods or override existing ones from the parent class.</p>
</blockquote>
<pre><code>class ElectricCar(Car):
    def charge(self):
        print(f&quot;The {self.color} {self.make} {self.model} is charging.&quot;)
</code></pre>
<p>ElectricCar is a <strong>subclass</strong> of Car. It inherits all the attributes and methods of <code>Car</code> and adds a new method, <code>charge()</code>.</p>
<blockquote>
<p>An <strong>abstract class</strong> is a class that cannot be instantiated on its own and is meant to be a base class for other classes. It often contains abstract methods, which are methods that are declared but contain no implementation. The <strong>subclasses</strong> of the abstract class must implement these abstract methods.
In other words, an abstract class is the <strong>MOTHER OF EVERYTHING</strong></p>
</blockquote>
<p>In Python, abstract classes are created using the <code>ABC</code> module, and abstract methods are marked with the <code>@abstractmethod</code> decorator.</p>
<pre><code class="language-py">from abc import ABC, abstractmethod

class Animal(ABC):
    @abstractmethod
    def sound(self):
        pass

class Dog(Animal):
    def sound(self):
        return &quot;Bark&quot;

my_dog = Dog()
print(my_dog.sound())  # Outputs: Bark
</code></pre>
<p>Here, <code>Animal</code> is an abstract class, and <code>sound()</code> is an abstract method that must be implemented by any non-abstract subclass like <code>Dog</code>.</p>
<p>Now onto the more complicated stuffs:</p>
<blockquote>
<p>The <strong>factory pattern</strong> is a design pattern that deals with the creation of objects. A factory class or method is responsible for creating objects without specifying the exact class of the object that will be created. It’s useful when you want to abstract the creation process of objects, especially when the object creation involves some complexity.</p>
</blockquote>
<pre><code class="language-py">class AnimalFactory:
    @staticmethod
    def create_animal(animal_type):
        if animal_type == &quot;dog&quot;:
            return Dog()
        elif animal_type == &quot;cat&quot;:
            return Cat()
        else:
            return None

my_pet = AnimalFactory.create_animal(&quot;dog&quot;)
print(my_pet.sound())  # Outputs: Bark
</code></pre>
<p>Here, <code>AnimalFactory</code> is a factory class that creates instances of <code>Dog</code> or <code>Cat</code> based on input.</p>
<blockquote>
<p>A <strong>controller</strong> is a class that manages the interaction between different parts of a program. It doesn’t handle the core logic itself but instead orchestrates the different pieces, making sure they work together.</p>
<p>[!quote] summary
different types of classes
- <strong>Abstract Classes</strong> are at the top, defining the structure.
- <strong>Concrete Classes</strong> implement these abstract classes.
- <strong>Factory and Controller Classes</strong> operate at the same level as concrete classes:
  - <strong>Factory Classes</strong> are responsible for creating objects (like concrete classes).
  - <strong>Controller Classes</strong> manage the workflow, using objects created by the factory.
  - <strong>Methods and Attributes</strong> are at the bottom, as the actual code and data that define the behavior of classes.</p>
</blockquote>
<h2 id="models">Models</h2>
<p>https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</p>
<h3 id="a-preprocessing">A. Preprocessing</h3>
<blockquote>
<p>from sklearn.preprocessing import ___
<strong>fit_transform(data)</strong></p>
</blockquote>
<h4 id="1-polynomial-features-overfitting">1. Polynomial features &amp; overfitting</h4>
<p><code>from sklearn.preprocessing import PolynomialFeatures</code> | for &gt; linear models</p>
<ol>
<li>Initialise model using <code>PolynomialFeatures</code></li>
</ol>
<pre><code>rolypoly = PolynomialFeatures(degree = 8, include_bias = False)
</code></pre>
<ol>
<li>Fit model using <code>fit_transform</code></li>
</ol>
<pre><code>transformed = rolypoly.fit_transform(data[[&quot;column1&quot;, &quot;column2&quot;, &quot;column3&quot;]])
</code></pre>
<p>Rmb in econometrics, we could add higher than linear terms,</p>
<ol>
<li><strong>Linear (Degree 1)</strong>: $ax+b$</li>
<li><strong>Quadratic (Degree 2)</strong>: $ax^2+bx+c$</li>
<li><strong>Cubic (Degree 3)</strong>: $ax^3+bx^2+cx+d$</li>
<li><strong>Quartic (Degree 4)</strong>: $ax^4+bx^3+cx^2+dx+e$</li>
<li><strong>Quintic (Degree 5)</strong>: $ax^5+bx^4+cx^3+dx^2+ex+f$</li>
<li><strong>Sextic (Degree 6)</strong>: $ax^6+bx^5+cx^4+dx^3+ex^2+fx+g$</li>
<li><strong>Septic (Degree 7)</strong>: $ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h$</li>
<li><strong>Octic (Degree 8)</strong>: $ax^8+bx^7+cx^6+dx^5+ex^4+fx^3+gx^2+hx+i$</li>
<li><strong>Nonic (Degree 9)</strong>: $ax^9+bx^8+cx^7+dx^6+ex^5+fx^4+gx^3+hx^2+ix+j$</li>
<li><strong>Decic (Degree 10)</strong>: $ax^{10}+bx^9+cx^8+dx^7+ex^6+fx^5+gx^4+hx^3+ix^2+jx+k$</li>
</ol>
<p>For degrees higher than ten, the naming often simply uses the ordinal number followed by "-ic" or "-degree polynomial," such as:</p>
<ul>
<li><strong>11th-degree polynomial</strong> or <strong>Undecic</strong></li>
<li><strong>12th-degree polynomial</strong> or <strong>Duodecic</strong></li>
</ul>
<pre><code>X[&quot;hp^3&quot;] = df[&quot;hp&quot;]**3   # cubic, fit a model with order 3
X[&quot;hp^4&quot;] = df[&quot;hp&quot;]**4
</code></pre>
<pre><code># short hand for X and y (defining them in fit)

# Fit a model with order 3
hp3_model = lm.LinearRegression()
hp3_model.fit(X[[&quot;hp&quot;, &quot;hp^2&quot;, &quot;hp^3&quot;]], vehicles[&quot;mpg&quot;])
hp3_model_predictions = hp3_model.predict(X[[&quot;hp&quot;, &quot;hp^2&quot;, &quot;hp^3&quot;]])

# Fit a model with order 4
hp4_model = lm.LinearRegression()
hp4_model.fit(X[[&quot;hp&quot;, &quot;hp^2&quot;, &quot;hp^3&quot;, &quot;hp^4&quot;]], vehicles[&quot;mpg&quot;])
hp4_model_predictions = hp4_model.predict(X[[&quot;hp&quot;, &quot;hp^2&quot;, &quot;hp^3&quot;, &quot;hp^4&quot;]])
</code></pre>
<p>The problem is <strong>model complexity</strong> - the more features we have, the lower values of training error. In fact, it is a mathematical fact that if we create a polynomial model with degree $n-1$, we can <em>perfectly</em> model a set of $n$ points. For example, a set of 5 points can be perfectly modeled by a degree 4 model.</p>
<p>![[Pasted image 20240514160502.png|center|70%]]</p>
<p>However, obviously there's gonna be a trade-off: <strong>higher test error</strong>. The model has <strong>overfit</strong> to the data used to train it. It has essentially "memorized" the six datapoints used during model fitting, and does not generalize well to new data.</p>
<p>Complex models tend to be more sensitive to the data used to train them. The <strong>variance</strong> of a model refers to its tendency to vary depending on the training data used during model fitting. It turns out that our degree-5 model has very high model variance. If we randomly sample new sets of datapoints to use in training, the model varies erratically.</p>
<p>![[under_over_fitting.png |center|40%]]</p>
<h4 id="2-standard-scaling">2. Standard scaling</h4>
<p>Scaling adjusts the range of features to a predefined scale, which can be either standardization or normalisation. When values = on vastly different scales -&gt; skew models -&gt; must adjust the ranges etc</p>
<blockquote>
<p>[!note]
<strong>Standardisation</strong> = rescales features to have a mean of 0 and a standard deviation of 1.
* Standardisation is useful when the features have different scales, and you want to give them equal weight in the model. It is particularly beneficial for algorithms that assume Gaussian distributions, such as linear regression and logistic regression.</p>
<p><strong>Normalisation</strong> rescales features to a fixed range
* range is usu 0 to 1
* Normalization preserves the shape of the original distribution while bringing the values into a similar numerical range. It is useful when the scale of the features is not known and might vary widely.
* e.g. min max scaler</p>
<p>[!note]
<strong>Standardisation</strong> = rescales features to have a mean of 0 and a standard deviation of 1.
* Standardisation is useful when the features have different scales, and you want to give them equal weight in the model. It is particularly beneficial for algorithms that assume Gaussian distributions, such as linear regression and logistic regression.</p>
<p><strong>Normalisation</strong> rescales features to a fixed range
* range is usu 0 to 1
* Normalization preserves the shape of the original distribution while bringing the values into a similar numerical range. It is useful when the scale of the features is not known and might vary widely.</p>
<p>e.g. min max scaler</p>
</blockquote>
<pre><code class="language-py">from sklearn.preprocessing import StandardScaler
fit.transform
</code></pre>
<p><strong>Use cases</strong>
* before regularisation</p>
<blockquote>
<p>[!warning] We do NOT scale y_train, only</p>
</blockquote>
<h3 id="b-model-selection">B. Model selection</h3>
<blockquote>
<p>from sklearn.model_selection import ___
<strong>fit(x_train, y_train)</strong></p>
</blockquote>
<h4 id="1-train-test-split">1. Train test split</h4>
<p><code>from sklearn.model_selection import train_test_split</code></p>
<p>y_train contains the target output corresponding to X_train values</p>
<pre><code class="language-py">Age    Sex       Disease
----  ------ |  ---------

  X_train    |   y_train   )
                           )
 5       F   |  A Disease  )
 15      M   |  B Disease  ) 
 23      M   |  B Disease  ) training
 39      M   |  B Disease  ) data
 61      F   |  C Disease  )
 55      M   |  F Disease  )
 76      F   |  D Disease  )
 88      F   |  G Disease  )
-------------|------------

  X_test     |    y_test

 63      M   |  C Disease  )
 46      F   |  C Disease  ) test
 28      M   |  B Disease  ) data
 33      F   |  B Disease  )
</code></pre>
<p>We <strong>fit</strong> the model on <strong>X_train</strong> and <strong>y_train</strong>: <code>lalala.fit(X_train, y_train)</code></p>
<p>We <strong>predict</strong> on <strong>X_test</strong>: <code>lalala.predict(X_test)</code></p>
<p>To get the <strong>accuracy score</strong>: <code>lalala.score</code> (general purpose, which returns <code>r2_score</code> for regression model and <code>accuracy_score</code> for classification model)</p>
<ul>
<li>for training accuracy score: <code>lalala.score(X_train, y_train)</code></li>
<li>test_score = lalala<code>.score(X_test, y_test)</code></li>
</ul>
<p><strong>Argument: <code>stratify = True</code></strong>
With shuffling, random order. You can stratify by defining the number of samples by class, when splitting, to stay proportional</p>
<pre><code class="language-py">from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                                    test_size=0.2, 
                                                    random_state=100, 
                                                    shuffle=True)
</code></pre>
<h4 id="2-one-hot-encoder">2. One-hot encoder</h4>
<p>It is essentially creating dummy variables (January = 0/1, February = 0/1, etc creating 11 dummy variables <code>from sklearn.preprocessing import OneHotEncoder</code></p>
<p>![[ohe.png | center]]</p>
<p>==step 1== initialise a one hot encoder (ohe) object.</p>
<pre><code>ohe = OneHotEncoder()
</code></pre>
<p>==step 2== fit the encoder onto the column needs encoding (categorical column)</p>
<pre><code>ohe.fit(df[['column']])
</code></pre>
<p>==step 3== transform the categorical column using <code>transform</code> and <code>get_feature_names_out</code></p>
<pre><code>encoded_array = ohe.transform(df[['column]]).toarray()
encoded_df = pd.DataFrame(encoded_array, columns = ohe.get_feature_names_out())
</code></pre>
<p>==step 4== rejoin with the full X dataframe, drop the categorical column</p>
<pre><code>X = X_raw.join(encoded_df).drop(columns = 'day')
</code></pre>
<h3 id="c-models">C. Models</h3>
<blockquote>
<p>import sklearn.linear_model as ___</p>
</blockquote>
<h4 id="1-linear-regression">1. Linear regression</h4>
<p>==step 1== <strong>Model initialisation</strong> - initialise an instance of the model class, in this case, <code>LinearRegression()</code></p>
<pre><code>import sklearn.linear_model as lalalalalala
my_model = lalalalalala.LinearRegression()
</code></pre>
<p>==step 2== <strong>Train the model using <code>.fit</code></strong></p>
<p>BTS, sklearn will run gradient descent to determine the optimal model params -&gt; save these model params to our model instance for future use</p>
<blockquote>
<p>All sklearn model classes include a <code>.fit</code> method, which takes 2 inputs:
1. <strong>the design matrix / predictor vars / independent vars (X)</strong>
2. <strong>the target variable / predicted (y)</strong></p>
</blockquote>
<p>.fit expects to receive 2D data thus it should be dataframe[['column']], not dataframe[\'column']</p>
<pre><code>my_model.fit(X, Y)
</code></pre>
<p>==step 3== necessary methods</p>
<p><code>.intercept_</code> = intercept</p>
<blockquote>
<p>[!NOTE] Quick way to extract the coefficients
[theta_1, theta_2, ...], $theta_0$ = model_h .coef_, model_h .intercept_</p>
</blockquote>
<p><code>.coef_</code> = all x coefficients</p>
<p><code>.predict</code> = predicted y</p>
<blockquote>
<p>[!success] Short form (1 line)
modelll = LinearRegression().fit(AQS, PA)</p>
</blockquote>
<h4 id="2-regularisation">2. Regularisation</h4>
<p>Essentially they are linear regression but with shrinkage (LinearRegression = regularisation = 0)</p>
<blockquote>
<p>[!warning]+ Must standardise data before regularisation</p>
</blockquote>
<p><strong>L1 regularisation (LASSO)</strong></p>
<pre><code>lasso_model_scaled = lm.Lasso(alpha=0.1)
lasso_model_scaled.fit(X_train_standardized, Y_train)
lasso_model_scaled.coef_
</code></pre>
<p><strong>L2 regularisation (RIDGE)</strong></p>
<pre><code>ridge_model = lm.Ridge(alpha=0.1)
ridge_model.fit(X_train_standardized, Y_train)
ridge_model.coef_
</code></pre>
<h3 id="d-cross-validation">D. Cross validation</h3>
<p>cross validation score: <code>from sklearn.model_selection import cross_val_score</code></p>
<p>kfold: <code>from sklearn.model_selection import KFold</code></p>
<ol>
<li>Initialise cross validation method</li>
</ol>
<pre><code class="language-py">cross_val_model = KFold(
    n_splits = 888,
    random_state = None,
    shuffle = False)
</code></pre>
<ol>
<li>(optional) identify train and test indices</li>
</ol>
<pre><code class="language-py"># cross_val_model.split(X)

for train_idx, valid_idx in kf.split(X_train):
    split_X_train, split_X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]
    split_y_train, split_y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]
</code></pre>
<ol>
<li>(optional) to visualise the folds</li>
</ol>
<pre><code class="language-py">for i, (train_index, test_index) in enumerate(kf.split(X)):
    print(f&quot;Fold {i}:&quot;)
    print(f&quot;  Train: index={train_index}&quot;)
    print(f&quot;  Test:  index={test_index}&quot;)
</code></pre>
<ol>
<li>model fit and calculating mean error for each fold</li>
</ol>
<pre><code class="language-py">ML_model = LinearRegression()
scores = cross_val_score(ML_model, X, y, scoring = 'accuracy' cv = cross_val_model, n_jobs = -1)
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))
</code></pre>
<pre><code class="language-py"># before looping over each train/test index
validation_errors = []

# in each loop
moddell.fit(X_train, y_train)
error = ...
validation_errors.append(error)

# outside of loop
mean_error_loops = np.mean(validation_errors)
</code></pre>
<p><strong>Methods</strong>
kalulu<code>.get_n_splits(X)</code> - generate indices to split data into training and test set</p>
<h2 id="visualisation">visualisation</h2>
<p>Add <code>;</code> to remove unnecessary text lines when drawing using sns (on the same line as sns lines not at the end of the code block on a separate line)</p>
<p>Use <code>sns.countplot</code> instead of barplots when you don't have the variable name for one of the axes (which is most of the time)</p>
<h3 id="graph-styling">graph styling</h3>
<pre><code>import seaborn as sns
import pandas as pd
sns.set(font_scale=1.5)
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import plotly.graph_objects as go
import plotly.express as px
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

def adjust_fontsize(size=None):
    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12
    if size != None:
        SMALL_SIZE = MEDIUM_SIZE = BIGGER_SIZE = size

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

plt.style.use('fivethirtyeight')
sns.set_context(&quot;talk&quot;)
sns.set_theme()
adjust_fontsize(size=20)
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')
</code></pre>
<h3 id="helper-functions">helper functions</h3>
<p>Plot distribution bar charts</p>
<pre><code class="language-py">def plot_dist(dist_df,
                      xname=&quot;x&quot;, pname=&quot;P(X = x)&quot;, varname=&quot;X&quot;,
                      save=False):
    &quot;&quot;&quot;
    Plot a distribution from a distribution table.
    Single-variate.
    &quot;&quot;&quot;
    plt.bar(dist_df[xname], dist_df[pname])
    plt.ylabel(pname)
    plt.xlabel(xname)
    plt.title(f&quot;Distribution of ${varname}$&quot;)
    plt.xticks(sorted(dist_df[xname].unique()))
    if save:
        fig = plt.gcf()
        fig.patch.set_alpha(0.0)
        plt.savefig(f&quot;dist{varname}.png&quot;, bbox_inches = 'tight');
</code></pre>
<p>Simulate random samples</p>
<pre><code class="language-py">def simulate_samples(df, xname=&quot;x&quot;, pname=&quot;P(X = x)&quot;, size=1):
    return np.random.choice(
                df[xname], # Draw from these choiecs
                size=size, # This many times
                p=df[pname]) # According to this distribution
</code></pre>
<h3 id="subplots">subplots</h3>
<pre><code class="language-py">fig, ax = plt.subplots(1, 3, figsize=(12, 3))

# looping 

predictions_dict = {0:hp2_model_predictions, 1:hp3_model_predictions, 2:hp4_model_predictions}

for i in predictions_dict:
    ax[i].scatter(vehicles[&quot;hp&quot;], vehicles[&quot;mpg&quot;], edgecolor=&quot;white&quot;, lw=0.5)
    ax[i].plot(vehicles[&quot;hp&quot;], predictions_dict[i], &quot;tab:red&quot;)
    ax[i].set_title(f&quot;Model with order {i+2}&quot;)
    ax[i].set_xlabel(&quot;hp&quot;)
    ax[i].set_ylabel(&quot;mpg&quot;)
    ax[i].annotate(f&quot;MSE: {np.round(mse(vehicles['mpg'], predictions_dict[i]), 3)}&quot;, (120, 40))

plt.subplots_adjust(wspace=0.3);
</code></pre>
<pre><code class="language-py">fig, ax = plt.subplots(ncols=2)
sns.histplot(tips['tip'], bins=20, stat=&quot;proportion&quot;, ax=ax[0])
sns.histplot(tips['tip']/tips['total_bill'], bins=20, stat=&quot;proportion&quot;, ax=ax[1])
</code></pre>
<h2 id="finance">Finance</h2>
<blockquote>
<p>[!NOTE] Finding percentage change / return of strategy</p>
<ol>
<li>Long way = t1 / t0 - 1
```</li>
</ol>
</blockquote>
<p>spy['returns'] = spy['close'].shift(21) / spy['close'] - 1</p>
<blockquote>
<p><code>2. Short way</code></p>
</blockquote>
<p>spy['return'] = spy.close.shift(-21).pct_change(21)</p>
<p>spy['strategy_return'] = spy['return'].mul(spy.signal_momentum_high)</p>
<blockquote>
<p>```</p>
</blockquote>
<h2 id="dataframe">Dataframe</h2>
<p><code>df1.equals(df2)</code> to check if 2 dataframes are equal</p>
<p><strong>To join/merge/append new columns</strong>
1. join
2. merge (more general and versatile)</p>
<p><strong>To display all column names in a pretty way</strong>
Instead of df.columns:</p>
<pre><code>display(Markdown('\n'.join(fr'{i + 1}. {c}'.replace('$', r'\$') for i, c in enumerate(wb.columns))))
</code></pre>
<h2 id="multiprocessing">Multiprocessing</h2>
<p><strong>Joblib parallel</strong>: In rough terms, it spawns multiple Python processes and handles each part of the iterable in a separate process. Then it joins everything at the end. ==see backtesting paper - n_jobs==</p>
<p>Examples:</p>
<pre><code class="language-py">results = Parallel(n_jobs=n_jobs)(
    delayed(FUNCTION)(FUNCTION ARGUMENTS)
    for random_state_value in random_states
)
</code></pre>
<pre><code class="language-py">path_data = Parallel(n_jobs=n_jobs)(
    delayed(train_test_single_estimator)(
        deepcopy(single_estimator), train_indices, test_indices
    ) for train_indices, test_indices in self._single_split(single_data)
)
</code></pre>
<pre><code class="language-py">results = Parallel(n_jobs=n_jobs)(
    delayed(performance_evaluation)(
        np.concatenate([partitions[i] for i in train_indices], axis=0),
        np.concatenate([partitions[i] for i in partition_indices 
                        if i not in train_indices], axis=0),
        n_strategies, 
        metric, 
        risk_free_return
    ) 
    for train_indices in partition_combinations_indices
)
</code></pre>
<h2 id="pipeline">Pipeline</h2>
<p><code>from sklearn.pipeline import Pipeline</code></p>
<p>Example: fitting polynomial model 18 times</p>
<pre><code class="language-py">def fit_model_dataset(degree):
    pipelined_model = Pipeline([
        ('polynomial_transformation', 'PolynomialFeatures(degree)),
        ('linear_regression', lm.LinearRegression())
    ])
    pipelined_model.fit(X_train[['hp']], Y_train)
    return mean_squared_error(Y_value, pipelined_model.predict(X_value[['hp']]))
</code></pre>
<h2 id="miscellaneous">Miscellaneous</h2>
<h3 id="simple-boolean-if-xyz-prints-1">simple boolean, if xyz, prints 1</h3>
<pre><code class="language-py">ins['Missing score'] = ins['score'] == -1
OR
ins['Missing score'] = (ins['score'] == -1)
</code></pre>
<h3 id="replace">replace</h3>
<p>If I have a table called wine, class column:</p>
<p>-&gt; To know how many unique values there are in class: <code>wine['class'].unique()</code></p>
<p>-&gt; To replace all values not 1 into 0: <strong>wine['class']</strong><code>.apply(lambda x: 1 if x == 1 else 0)</code></p>
<h3 id="normalise-truetrue">Normalise = True[True]</h3>
<p>value_counts(normalize=True)</p>
<p>-&gt; convert counts to proportions</p>
<p>value_counts(normalize=True)[True]</p>
<p>-&gt; only show the True results</p>
<h3 id="2f">:.2f</h3>
<p>= display to 2 decimal places</p>
<p><code>relevant_attributes = movies[['Title', 'Genre'] + attribute_columns]</code></p>
<p>table then some extra columns = use plus sign</p>
<h3 id="for-codes-spanning-multiple-lines">For codes spanning multiple lines</h3>
<p>Use either:</p>
<ul>
<li>\ every line end OR</li>
<li>() on the RHS of equation</li>
<li>wrap all with ' '</li>
</ul>
<p>Line code &gt; 80 -&gt; use <strong>single quotes</strong> to wrap each line:</p>
<pre><code class="language-py">print(f'The predicted genre for the movie &quot;{train_set.loc[235, &quot;Title&quot;].title()}&quot;'
      f' is: {predicted_genre.loc[0, &quot;Genre&quot;].upper()}.')
</code></pre>
<pre><code class="language-py">k = int(input(&quot;Enter k: &quot;))
if isinstance(k, int) and k &gt; 0:
    predicted_genre = knn_classification(train_set.loc[235], train_set).reset_index()
    print(f'The predicted genre for the movie &quot;{train_set.loc[235, &quot;Title&quot;].title()}&quot;'
          f' is: {predicted_genre.loc[0, &quot;Genre&quot;].upper()}.')
else:
    print(&quot;Invalid input. Please enter a valid integer for k.&quot;)
</code></pre>
<h3 id="in-paths-work-unless-its-followed-by-n">in paths work unless it's followed by n</h3>
<pre><code class="language-py">pd.read_csv(&quot;data\nst-est2019-01.csv&quot;) # would NOT work cos \n
</code></pre>
<p>Solution: add <code>r</code> at the front -&gt; indicates that it's a raw string, and backslashes will be treated as literal characters.</p>
<pre><code class="language-py">pd.read_csv(&quot;data/nst-est2019-01.csv&quot;)
</code></pre>
<p>Or simply use a forward slash /</p>
<h3 id="npsearchsorted">np.searchsorted</h3>
<p><code>numpy.searchsorted(a, v, side='left', sorter=None)</code></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../Doccen%20policy%20update/" class="btn btn-neutral float-left" title="Doccen policy update"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../Maths/DATA101%20Data%20engineering%20PostgreSQL/" class="btn btn-neutral float-right" title="DATA101 Data engineering PostgreSQL">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../Doccen%20policy%20update/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../Maths/DATA101%20Data%20engineering%20PostgreSQL/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
